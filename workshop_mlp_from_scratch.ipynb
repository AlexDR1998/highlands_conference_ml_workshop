{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network from scratch\n",
    "\n",
    "in this notebook we will:\n",
    "- define a simple neural network in the JAX library\n",
    "- define a loss function to measure its performance\n",
    "- perform a simple stochastic gradient descent to optimise the network parameters by minimising the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax # The main library we'll be using here\n",
    "import jax.numpy as np # The entire numpy library, but differentiable\n",
    "import jax.random as jr # Pseudorandom number generators\n",
    "import einops # Fancy reshaping of arrays\n",
    "from tqdm.notebook import tqdm # Loading bars for our loops\n",
    "import matplotlib.pyplot as plt # Visualisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We load the mnist handwritten digit dataset:\n",
    "- x_train: (60000,28,28) array of images (28*28 resolution)\n",
    "- x_test: (10000,28,28) array of images (28*28 resolution)\n",
    "\n",
    "- y_train: (60000) array of labels (integers 0-9)\n",
    "- y_test: (10000) array of labels (integers 0-9)\n",
    "\n",
    "- y_train_1hot: (60000,10) array of one hot encoded labels\n",
    "- y_test_1hot: (10000,10) array of one hot encoded labels\n",
    "\n",
    "We should always visualise what our data actually looks like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only use tensorflow for loading the mnist dataset \n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train_flat = einops.rearrange(x_train,\"batches w h -> batches (w h)\") / 255.0\n",
    "x_test_flat = einops.rearrange(x_test,\"batches w h -> batches (w h)\") / 255.0\n",
    "y_train_1hot = jax.nn.one_hot(y_train,num_classes=10,axis=-1)\n",
    "y_test_1hot = jax.nn.one_hot(y_test,num_classes=10,axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Jax \n",
    "- JAX is a python library that essentially reproduces numpy, but with 3 very useful additions:\n",
    "    - everything is automatically differentiable\n",
    "    - code automatically parallelises to GPUs (when applicable)\n",
    "    - code can be Just In Time (JIT) compiled to run much faster than python typically does\n",
    "- the `jax.numpy` module has the same API as numpy, so if you replace `import numpy as np` with `import jax.numpy as np` your code will probably work\n",
    "- All random methods in JAX require a PRNGKey object passed into them\n",
    "- This is to avoid side effects, keeping functions that rely on random numbers pure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "key = jr.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear fully connected layers:\n",
    "- The linear layers of a fully connected feed-forward neural network (or multi-layer-perceptron) are just matrix multiplications followed by adding a bias vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(x,weights,bias):\n",
    "    return np.matmul(weights,x) + bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets define some nonlinear activation functions:\n",
    "- relu (REctified Linear Unit) just sets negative values to 0. It's fast to compute and works fairly well\n",
    "- softmax normalises the output to be a probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_activation(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "def softmax_activation(x):\n",
    "    Z = np.sum(np.exp(x))\n",
    "    return np.exp(x)/Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now define a neural network as a python class:\n",
    " - `__init__` method defines the sizes of input, hidden and output layers\n",
    " - `__call__` allows for objects of this class to be called like functions\n",
    " - multiplying the normally distributed parameters by `0.01` in `__init__` is important\n",
    "    - if initial random parameters are too large, training tends to diverge to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(object):\n",
    "    def __init__(self,input_size,hidden_size,output_size,key):\n",
    "        key1,key2,key3,key4 = jr.split(key,4) # PRNGKey used to initialise model parameters\n",
    "        self.weights = [jr.normal(key1,shape=(hidden_size,input_size))*0.01,\n",
    "                        jr.normal(key2,shape=(output_size,hidden_size))*0.01]\n",
    "        self.bias = [jr.normal(key3,shape=(hidden_size,))*0.01,\n",
    "                     jr.normal(key4,shape=(output_size,))*0.01]\n",
    "        \n",
    "        \n",
    "    def __call__(self,x):\n",
    "        \n",
    "        h1 = linear_layer(x,self.weights[0],self.bias[0])\n",
    "        h2 = relu_activation(h1)\n",
    "        h3 = linear_layer(h2,self.weights[1],self.bias[1])\n",
    "        h4 = softmax_activation(h3)\n",
    "        return h4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = SimpleNetwork(784,100,10,key)\n",
    "print(nn(x_train_flat[4]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now define a loss function - this is the bit with the gradient:\n",
    "- `loss` takes as input model_parameters, model, x and y, where:\n",
    "    - model_parameters: is a list of arrays representing the weights and biasses\n",
    "    - model: callable (model_parameters,x)->y is the neural network function itself\n",
    "    - x: array [BATCHES,784] is the input data\n",
    "    - y: array [BATCHES,10] is the correct outputs to compare to\n",
    "- loss returns a scalar, think of this as a measure of how wrong the model is\n",
    "    - This particular loss function is the cross entropy, which works well for comparing probability distributions\n",
    "- the `@jax.value_and_grad` wrapper means that loss returns both it's normal output, and the gradient of this output __with respect to it's first input__ (model_parameters)\n",
    "\n",
    "- the `jax.vmap` vectorises the model function - so it works on [BATCHES,748] inputs rather than just [748] inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.value_and_grad\n",
    "def loss(model_params,model,x,y):\n",
    "    \"\"\"Loss function\n",
    "\n",
    "    Args:\n",
    "        model_parameters (list of arrays): parameters of the network model\n",
    "        model (callable (model_parameters,x)->y ): neural network model being trained\n",
    "        x (array [BATCHES,1,28,28]): image of number\n",
    "        y (array [BATCHES,10]): one hot encoding of number label\n",
    "\n",
    "    Returns:\n",
    "        float: loss - zero when y_pred=y, higher otherwise\n",
    "    \"\"\"\n",
    "    model.weights = [model_params[0],model_params[1]]\n",
    "    model.bias = [model_params[2],model_params[3]]\n",
    "    \n",
    "    y_predicted = jax.vmap(model,in_axes=0,out_axes=0,axis_name=\"BATCHES\")(x)\n",
    "    return -np.mean(y*np.log(y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we define an optimiser:\n",
    "- This is just a function that updates the model parameters based on the loss gradients\n",
    "- To minimise the loss we use a 1st order gradient descent:\n",
    "    -  $\\theta_{i+1} = \\theta_i - h\\nabla_\\theta L(\\theta,x,y)$\n",
    "    - where $h$ is the `learn_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimiser(model_parameters,gradient,learn_rate):\n",
    "    # Simple gradient descent step\n",
    "    for i,dm in enumerate(gradient):\n",
    "        model_parameters[i] -= learn_rate*dm\n",
    "    return model_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we stick everything together:\n",
    "- iteratively:\n",
    "    - calculate loss gradients of the model\n",
    "    - update the model parameters based on these gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set network and training log\n",
    "key = jr.fold_in(key,1)\n",
    "nn = SimpleNetwork(28*28,100,10,key)\n",
    "nn_params = [nn.weights[0],nn.weights[1],nn.bias[0],nn.bias[1]]\n",
    "TRAIN_ITERS = 1000\n",
    "BATCH_SIZE = 64\n",
    "loss_log = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(TRAIN_ITERS)):\n",
    "    \n",
    "    # Randomly choose 32 pairs of X and Y\n",
    "    train_key = jr.fold_in(key,i)\n",
    "    inds = jr.choice(train_key,np.arange(60000),(BATCH_SIZE,),replace=False)\n",
    "    x = x_train_flat[inds]\n",
    "    y = y_train_1hot[inds]\n",
    "    \n",
    "    loss_value,grad = loss(nn_params,nn,x,y) # Compute loss\n",
    "    nn_params = optimiser(nn_params,grad,learn_rate=1e-1) # Apply gradient updates to model parameters\n",
    "    \n",
    "    \n",
    "    loss_log.append(loss_value) # Record current loss\n",
    "    if i%10==0:\n",
    "        tqdm.write(\"Loss at step \"+str(i)+\": \"+str(loss_value)) # Print loss to screen during training\n",
    "\n",
    "\n",
    "nn.weights = nn_params[0],nn_params[1] # Set the nn weights and biasses to the learned parameters\n",
    "nn.bias = nn_params[2],nn_params[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check how the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's evaluate our trained model on the test dataset\n",
    "- A confusion matrix is a useful way to visualise this\n",
    "    - each entry $C_{ij}$ counts how many times an image of class $i$ was labelled as class $j$\n",
    "    - If our model is perfect, this matrix should be diagonal\n",
    "    - It can be helpful to understand if there are two classes of images that are often confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predictions = np.argmax(jax.vmap(nn)(x_test_flat),axis=1) # By taking the maximum of the output probability distribution, we are forcing the model to discretely choose a number\n",
    "confusion_matrix = np.zeros((10,10))\n",
    "for i in range(len(y_test)):\n",
    "    confusion_matrix = confusion_matrix.at[y_test[i],y_test_predictions[i]].set(confusion_matrix[y_test[i],y_test_predictions[i]]+1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What next?\n",
    "- This model is pretty bad\n",
    "- having to explictely pass around model parameters and model is annoying\n",
    "- having to define our own optimiser is annoying\n",
    "- How do we choose good hyper-paramters:\n",
    "    - number of layers\n",
    "    - sizes of layers\n",
    "    - types of activation function\n",
    "- Training sometimes hits NaN - dependant on random initialisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
