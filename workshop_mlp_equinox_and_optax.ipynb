{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple neural network with Equinox and Optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import jax.random as jr\n",
    "import optax\n",
    "import einops\n",
    "import equinox as eqx \n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We load the mnist handwritten digit dataset:\n",
    "- x_train: (60000,28,28) array of images (28*28 resolution)\n",
    "- x_test: (10000,28,28) array of images (28*28 resolution)\n",
    "\n",
    "- y_train: (60000) array of labels (integers 0-9)\n",
    "- y_test: (10000) array of labels (integers 0-9)\n",
    "\n",
    "- y_train_1hot: (60000,10) array of one hot encoded labels\n",
    "- y_test_1hot: (10000,10) array of one hot encoded labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only use tensorflow for loading the mnist dataset \n",
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train_flat = einops.rearrange(x_train,\"batches w h -> batches (w h)\") / 255.0\n",
    "x_test_flat = einops.rearrange(x_test,\"batches w h -> batches (w h)\") / 255.0\n",
    "y_train_1hot = jax.nn.one_hot(y_train,num_classes=10,axis=-1)\n",
    "y_test_1hot = jax.nn.one_hot(y_test,num_classes=10,axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a neural network by inheriting from `eqx.Module`:\n",
    "- This simplifies the syntax of JAX gradient calculations\n",
    "- `eqx.Module`:\n",
    "    - Classes inheriting from `eqx.Module` will be a python `dataclass`, meaning you have to define any self.variables at the class level, i.e. the `layers: list` before the `__init__`. This just forces you to be a bit cleaner in your software engineering\n",
    "    - See https://docs.kidger.site/equinox/api/module/module/\n",
    "- `eqx.nn`:\n",
    "    - `eqx.nn.Linear` constructs a linear layer, but there are many other layers included in this library\n",
    "    - See https://docs.kidger.site/equinox/api/nn/linear/\n",
    "- Notice that we also have some `jax.nn` functions. In general, any pure jax function will work in an eqx.Module model, and will be treated correctly during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,output_size,key):\n",
    "        key1, key2 = jax.random.split(key, 2)\n",
    "        self.layers = [\n",
    "            eqx.nn.Linear(input_size,hidden_size,key=key1),\n",
    "            jax.nn.relu,\n",
    "            eqx.nn.Linear(hidden_size,output_size,key=key2),\n",
    "            jax.nn.softmax\n",
    "        ]\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        for L in self.layers:\n",
    "            x = L(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `loss` function doesn't need explicit model parameters as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_value_and_grad\n",
    "def loss(model,x,y):\n",
    "    y_predicted = jax.vmap(model,in_axes=0,out_axes=0,axis_name=\"BATCHES\")(x)\n",
    "\n",
    "    #print(y_predicted)\n",
    "    #return np.mean((y_predicted-y)**2)\n",
    "    return -np.mean(y*np.log(y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop is a bit more involved now:\n",
    "- the `@eqx.filter_jit` wrapper around the `make_step` function performs Just In Time compilation\n",
    "    - this will run slower on the first call, but on every subsequent call will run much faster than normal python\n",
    "- `optax.adam` is a more sophisticated optimiser than the one we wrote\n",
    "    - the expression `optim.init(eqx.filter(model, eqx.is_array))` initialises the optimiser on the parts of model that are arrays\n",
    "    - the optimiser also has it's own internal state (to keep track of momentum). This is passed around as the `opt_state` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,steps,key,LEARN_RATE=1e-3,BATCH_SIZE=32):\n",
    "\n",
    "    optim = optax.adam(LEARN_RATE)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array))\n",
    "    loss_log = []\n",
    "    \n",
    "    @eqx.filter_jit\n",
    "    def make_step(model,opt_state,X,Y):\n",
    "        loss_value,grad = loss(model,X,Y)\n",
    "        updates, opt_state = optim.update(grad, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value\n",
    "    \n",
    "    #train_key = jr.fold_in(key,1)\n",
    "    for i in tqdm(range(steps)):\n",
    "        train_key = jr.fold_in(key,i)\n",
    "        inds = jr.choice(train_key,np.arange(60000),(BATCH_SIZE,),replace=False)\n",
    "        x = x_train_flat[inds]\n",
    "        y = y_train_1hot[inds]\n",
    "        #print(x.shape)\n",
    "        \n",
    "        model, opt_state, train_loss = make_step(model, opt_state, x, y)\n",
    "        \n",
    "        \n",
    "        loss_log.append(train_loss)\n",
    "        if i%10==0:\n",
    "            tqdm.write(\"Loss at step \"+str(i)+\": \"+str(train_loss))\n",
    "    plt.plot(loss_log)\n",
    "    plt.show()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.PRNGKey(12345)\n",
    "nn = SimpleNetwork(784,100,10,key)\n",
    "nn = train(nn,1000,key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's evaluate our trained model on the test dataset\n",
    "- A confusion matrix is a useful way to visualise this\n",
    "    - each entry $C_{ij}$ counts how many times an image of class $i$ was labelled as class $j$\n",
    "    - If our model is perfect, this matrix should be diagonal\n",
    "    - It can be helpful to understand if there are two classes of images that are often confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predictions = np.argmax(jax.vmap(nn)(x_test_flat),axis=1) # By taking the maximum of the output probability distribution, we are forcing the model to discretely choose a number\n",
    "confusion_matrix = np.zeros((10,10))\n",
    "for i in range(len(y_test)):\n",
    "    confusion_matrix = confusion_matrix.at[y_test[i],y_test_predictions[i]].set(confusion_matrix[y_test[i],y_test_predictions[i]]+1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
